{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "PADDING_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "CHECKPOINT_PATH_REL = \"training_rel/weights.ckpt\"\n",
    "SAMPLE_SIZE = 0.5\n",
    "EMBEDDING_DIMENSIONS = 50  # based on glove\n",
    "PATIENCE = 5\n",
    "VECTOR_VALUE = 4.0\n",
    "DATASET = \"webnlg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 21:57:42.536361: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-20 21:57:42.760003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 21:57:43.705950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 21:57:46.920480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:46.973745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:46.973955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:46.974815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:46.974989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:46.975136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:47.711936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:47.712088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:47.712196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 21:57:47.712305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3368 MB memory:  -> device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "400000it [00:37, 10621.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### vocab size 400000\n",
      "3.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "\n",
    "class_counter = {}\n",
    "\n",
    "vectors = {}\n",
    "with tf.device('/CPU:0'):\n",
    "    with open(\"glove.6B.\" + str(EMBEDDING_DIMENSIONS) + \"d.txt\", \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            vals = line.rstrip().split(\" \")\n",
    "            vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "dict_keys = list(vectors.keys())\n",
    "print(\"### vocab size\", len(vectors.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(word):\n",
    "    case_vector = VECTOR_VALUE if word[0].isupper() else (-1 * VECTOR_VALUE)\n",
    "    wrd = str(word).lower()\n",
    "    if wrd in vectors:\n",
    "        return vectors[wrd] + [case_vector]\n",
    "\n",
    "    char_embeddings = [\n",
    "        vectors.setdefault(c, vectors[dict_keys[ord(c) % 100]]) for c in wrd\n",
    "    ]\n",
    "    averaged_embeddings = []\n",
    "    tot = 0\n",
    "    for i in range(EMBEDDING_DIMENSIONS):\n",
    "        for j in range(len(char_embeddings)):\n",
    "            tot += char_embeddings[j][i]\n",
    "        averaged_embeddings.append(tot)\n",
    "\n",
    "    return averaged_embeddings + [case_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_relations(paths):\n",
    "    all_relations = []\n",
    "    for path in paths:\n",
    "        f = open(path)\n",
    "        corpus = json.load(f)\n",
    "        f.close()\n",
    "        for i in range(int(len(corpus) * 1)):\n",
    "            sentence_data = corpus[i]\n",
    "            for trpl in sentence_data[\"relation_list\"]:\n",
    "                rel = trpl[\"predicate\"]\n",
    "                all_relations.append(rel)\n",
    "\n",
    "    relations_list = list(set(all_relations))\n",
    "    print(f\"### Number of relations: {len(relations_list)} \\t\\t ###\")\n",
    "    return relations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(path: str, size = 1.0):\n",
    "    f = open(path)\n",
    "    corpus = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X = {\n",
    "        \"sentences\": [],\n",
    "        \"embeddings\": [],  # for relations task\n",
    "    }\n",
    "    Y = {\n",
    "        \"relations\": [],\n",
    "        \"relations_text\": [],\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(int(len(corpus) * size))):\n",
    "        sentence_data = corpus[i]\n",
    "        tokens = sentence_data[\"text\"].split(\" \")\n",
    "        X[\"sentences\"].append(sentence_data[\"text\"])\n",
    "        embeddings = []\n",
    "        relations_tagged = [0] * len(relations_vocab)\n",
    "        relations_text = []\n",
    "        sub_ranges = []\n",
    "        obj_ranges = []\n",
    "\n",
    "        for trpl in sentence_data[\"relation_list\"]:\n",
    "            rel = trpl[\"predicate\"]\n",
    "            relations_text.append(rel)\n",
    "            assert rel in relations_vocab, \"Relation should be found.\"\n",
    "            rel_index = relations_vocab.index(rel)\n",
    "            relations_tagged[rel_index] = 1\n",
    "\n",
    "            # subject ranges\n",
    "            sub_list = trpl[\"subject\"].split(\" \")\n",
    "            sub_head_index = tokens.index(sub_list[0])\n",
    "            sub_tail_index = tokens.index(sub_list[-1], sub_head_index)\n",
    "            assert sub_tail_index != -1\n",
    "            sub_ranges.append([sub_head_index, sub_tail_index])\n",
    "\n",
    "            # object ranges\n",
    "            obj_list = trpl[\"object\"].split(\" \")\n",
    "            obj_head_index = tokens.index(obj_list[0])\n",
    "            obj_tail_index = tokens.index(obj_list[-1], obj_head_index)\n",
    "            assert obj_tail_index != -1\n",
    "            obj_ranges.append([obj_head_index, obj_tail_index])\n",
    "\n",
    "        Y[\"relations\"].append(relations_tagged)\n",
    "        # remove duplicates\n",
    "        relations_text = list(set(relations_text))\n",
    "        Y[\"relations_text\"].append(\"**\".join(relations_text))\n",
    "\n",
    "        # iterate over words\n",
    "        for j, tkn in enumerate(tokens):\n",
    "            is_sub = is_obj = False\n",
    "            for s_rng in sub_ranges:\n",
    "                if j >= s_rng[0] and j <= s_rng[1]:\n",
    "                    is_sub = True\n",
    "                    break\n",
    "            for o_rng in obj_ranges:\n",
    "                if j >= o_rng[0] and j <= o_rng[1]:\n",
    "                    is_obj = True\n",
    "                    break\n",
    "\n",
    "            if is_sub:\n",
    "                embeddings.append(get_glove_embedding(tkn) + [VECTOR_VALUE])\n",
    "            elif is_obj:\n",
    "                embeddings.append(get_glove_embedding(tkn) + [-1 * VECTOR_VALUE])\n",
    "            else:\n",
    "                embeddings.append(get_glove_embedding(tkn) + [0])\n",
    "\n",
    "        padding_filling = (PADDING_SIZE - len(embeddings)) * [\n",
    "            (EMBEDDING_DIMENSIONS + 1 + 1) * [0.0]\n",
    "        ]\n",
    "        embeddings.extend(padding_filling)\n",
    "        X[\"embeddings\"].append(embeddings)\n",
    "\n",
    "    # to numpy to get memory space\n",
    "    for k in X.keys():\n",
    "        X[k] = np.array(X[k])\n",
    "        # X[k] = tf.constant(X[k])\n",
    "        print(\n",
    "            f\"{k}\\tshape\\t{ X[k].shape }\\tsize\\t{sys.getsizeof(X[k]) / 1024 / 1024:.4} MB\",\n",
    "        )\n",
    "\n",
    "    for k in Y.keys():\n",
    "        Y[k] = np.array(Y[k])\n",
    "        # Y[k] = tf.constant(Y[k])\n",
    "        print(\n",
    "            f\"{k}\\tshape\\t{ Y[k].shape }\\tsize\\t{sys.getsizeof(Y[k]) / 1024 / 1024:.4} MB\",\n",
    "        )\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_vocab = preprocess_relations(\n",
    "    [\n",
    "        \"data/\" + DATASET + \"/train.json\",\n",
    "        \"data/\" + DATASET + \"/valid.json\",\n",
    "        \"data/\" + DATASET + \"/test.json\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_X, train_Y = preprocess_data(\"data/\" + DATASET + \"/train.json\", SAMPLE_SIZE)\n",
    "valid_X, valid_Y = preprocess_data(\"data/\" + DATASET + \"/valid.json\", 1)\n",
    "test_X, test_Y = preprocess_data(\"data/\" + DATASET + \"/test.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, smooth=1e-6, gama=2):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.name = 'NDL'\n",
    "        self.smooth = smooth\n",
    "        self.gama = gama\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "        return tf.where(\n",
    "            tf.logical_and(\n",
    "                tf.equal(y_true, tf.constant(0.0)),\n",
    "                tf.logical_and(\n",
    "                tf.less(y_pred, tf.constant(0.5)),\n",
    "                tf.greater(y_pred, tf.constant(-1.0)),\n",
    "                )\n",
    "            ),\n",
    "            tf.divide(\n",
    "            # tf.divide(\n",
    "                #nominator\n",
    "                self.smooth ** 2,\n",
    "                #denominator\n",
    "                tf.reduce_sum(y_pred ** self.gama) + tf.reduce_sum(y_true ** self.gama) + self.smooth\n",
    "                )\n",
    "             , \n",
    "            1 - tf.divide(\n",
    "                #nominator\n",
    "                2 * tf.reduce_sum(tf.multiply(y_pred, y_true)) + self.smooth,\n",
    "                #denominator\n",
    "                tf.reduce_sum(y_pred ** self.gama) + tf.reduce_sum(y_true ** self.gama) + self.smooth\n",
    "                )\n",
    "        )\n",
    "\n",
    "    def call2(self, y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "        return 1 - tf.divide(\n",
    "                #nominator\n",
    "                2 * tf.reduce_sum(tf.multiply(y_pred, y_true)) + self.smooth,\n",
    "                #denominator\n",
    "                tf.reduce_sum(y_pred ** self.gama) + tf.reduce_sum(y_true ** self.gama) + self.smooth\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience, decay):\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_weights = None\n",
    "        self.noprogress_counter = 0\n",
    "        self.patience = patience\n",
    "        self.decay = decay\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        lr -= self.decay\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        print(f\"\\nLearning rate now is %6.6f.\" % (lr))\n",
    "\n",
    "        f1 = (2 * logs[\"val_precision\"] * logs[\"val_recall\"]) / (\n",
    "            logs[\"val_precision\"] + logs[\"val_recall\"] + 0.000001\n",
    "        )\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            print(f\"\\nValidation f1 {f1:.4} epoch {epoch}\")\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.noprogress_counter = 0\n",
    "\n",
    "        elif self.noprogress_counter > self.patience:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        else:\n",
    "            self.noprogress_counter += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relations_model(params):\n",
    "    inputs = layers.Input(shape=(PADDING_SIZE, EMBEDDING_DIMENSIONS + 1 + 1))\n",
    "\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            params[0],\n",
    "            return_sequences=True,\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "    )(inputs)\n",
    "\n",
    "    # value = SelfAttention(is_residual = True,\n",
    "    # attention_activation='relu'\n",
    "    #  )(bilstm)\n",
    "\n",
    "    avg1 = layers.AveragePooling1D(\n",
    "        pool_size=params[1], strides=params[2], padding=\"same\"\n",
    "    )(bilstm)\n",
    "\n",
    "    flt = layers.Flatten()(avg1)\n",
    "\n",
    "    dropout_layer = layers.Dropout(params[3])(flt)\n",
    "\n",
    "    output_layer = layers.Dense(\n",
    "        len(relations_vocab), name=\"rel_output\", activation=None\n",
    "    )(dropout_layer)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[inputs],\n",
    "        outputs=[\n",
    "            output_layer,\n",
    "        ],\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(\n",
    "            learning_rate=params[4],\n",
    "        ),\n",
    "        loss=DiceLoss(),\n",
    "        # loss = 'binary_crossentropy',\n",
    "        # loss = 'categorical_crossentropy',\n",
    "        # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[precision_metric, recall_metric],\n",
    "    )\n",
    "    print(params, model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_relations_training(params):\n",
    "    \n",
    "    np.random.seed(random.randint(100, 9999))\n",
    "    random.seed(random.randint(100, 9999))\n",
    "    tf.random.set_seed(random.randint(100, 9999))\n",
    "    relations_model = create_relations_model(params)\n",
    "    relationsCallback = CustomCallback(PATIENCE, (params[4] / EPOCHS))\n",
    "    history = relations_model.fit(\n",
    "        x=train_X[\"embeddings\"],\n",
    "        y=train_Y[\"relations\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            valid_X[\"embeddings\"],\n",
    "            valid_Y[\"relations\"],\n",
    "        ),\n",
    "        verbose=1,\n",
    "        callbacks=relationsCallback,\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    relations_model.save_weights(CHECKPOINT_PATH_REL)\n",
    "\n",
    "    # print(history.history, sep='\\n\\n####\\n\\n')\n",
    "\n",
    "    for k, v in enumerate(history.history.items()):\n",
    "        print(k)\n",
    "        print(v)\n",
    "\n",
    "    return None\n",
    "\n",
    "    res = relations_model.evaluate(\n",
    "        x=test_X[\"embeddings\"],\n",
    "        y=test_Y[\"relations\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    f1 = 2 * res[1] * res[2] / (res[1] + res[2])\n",
    "\n",
    "    print(f\"test f1 {f1:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_relations():\n",
    "    done_list = []\n",
    "\n",
    "    bilstm = [\n",
    "        500,\n",
    "    ]\n",
    "    avg1 = [50, 80, 100]\n",
    "    avg2 = [2, 3, 5, 7, 10]\n",
    "    drop = [\n",
    "        0.15,\n",
    "    ]\n",
    "    lr = [\n",
    "        0.0015,\n",
    "        0.001,\n",
    "        0.0005,\n",
    "    ]\n",
    "\n",
    "    for a in bilstm:\n",
    "        for b in avg1:\n",
    "            for e in lr:\n",
    "                for d in drop:\n",
    "                    for c in avg2:\n",
    "                        params = [a, b, c, d, e]\n",
    "\n",
    "                        if params in done_list:\n",
    "                            continue\n",
    "\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        np.random.seed(random.randint(100, 9999))\n",
    "                        random.seed(random.randint(100, 9999))\n",
    "                        tf.random.set_seed(random.randint(100, 9999))\n",
    "                        relations_model = create_relations_model(params)\n",
    "                        relationsCallback = CustomCallback(PATIENCE, (e / EPOCHS))\n",
    "                        relations_model.fit(\n",
    "                            x=train_X[\"embeddings\"],\n",
    "                            y=train_Y[\"relations\"],\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=EPOCHS,\n",
    "                            validation_data=(\n",
    "                                valid_X[\"embeddings\"],\n",
    "                                valid_Y[\"relations\"],\n",
    "                            ),\n",
    "                            verbose=0,\n",
    "                            callbacks=relationsCallback,\n",
    "                        )\n",
    "\n",
    "                        res = relations_model.evaluate(\n",
    "                            x=test_X[\"embeddings\"],\n",
    "                            y=test_Y[\"relations\"],\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                        )\n",
    "                        finish_time = time.time()\n",
    "                        print(\"params\", params)\n",
    "                        print(\"time\", str((finish_time - start_time) / 60))\n",
    "                        print(\"test f1 \", (2 * res[1] * res[2] / (res[1] + res[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params):\n",
    "    test_X, test_Y = preprocess_data(\"data/\" + DATASET + \"/test.json\", 1)\n",
    "\n",
    "    relations_model = create_relations_model(params)\n",
    "    relations_model.load_weights(CHECKPOINT_PATH_REL)\n",
    "    predictions = relations_model.predict(\n",
    "        x=test_X[\"embeddings\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    ground_truth_count = predicted_count = correct_count = 0\n",
    "\n",
    "    for i, ground_truth in enumerate(test_Y[\"relations_text\"]):\n",
    "        ground_truth_rel_list = ground_truth.split(\"**\")\n",
    "        ground_truth_count += len(ground_truth_rel_list)\n",
    "\n",
    "        for j, pred_probability in enumerate(predictions[i]):\n",
    "            if pred_probability >= 0.5:\n",
    "                predicted_count += 1\n",
    "                rel = relations_vocab[j]\n",
    "                if rel in ground_truth_rel_list:\n",
    "                    correct_count += 1\n",
    "\n",
    "    precision = correct_count / predicted_count\n",
    "    recall = correct_count / ground_truth_count\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    print(\n",
    "        f\"correct_count:{correct_count}, predicted_count:{predicted_count}, ground_truth_count:{ground_truth_count}\"\n",
    "    )\n",
    "    print(f\"precision :{precision}, recall :{recall}, f1 :{f1_score}\")\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    rel_model_params = [500,\n",
    "        80,\n",
    "        2,\n",
    "        0.15,\n",
    "        0.0015,\n",
    "    ]\n",
    "    run_relations_training(rel_model_params)\n",
    "    # tune_relations()\n",
    "    evaluate(rel_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#   tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=5)\n",
    "# model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
